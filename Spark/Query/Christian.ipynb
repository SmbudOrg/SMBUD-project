{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType, LongType\n",
    "from pyspark.sql.functions import col                       # Filtering using the col() function\n",
    "from pyspark.sql.functions import array_contains            # Filtering on array columns\n",
    "from pyspark.sql.functions import explode                   # Explode Arrays in Individual Rows\n",
    "from pyspark.sql.functions import sum, avg, count, max      # Multiple Aggregations\n",
    "from pyspark.sql.functions import first, last\t\n",
    "from pyspark.sql.functions import array_union               # Union of 2 array without duplicates\n",
    "from pyspark.sql.functions import lit, array\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"MyFirstSparkApplication\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT\n",
    "\n",
    "df_book = spark.read.option(\"multiline\",\"true\")  \\\n",
    "      .json(\"Datasets (json)/book-db.json\")\n",
    "\n",
    "df_article = spark.read.option(\"multiline\",\"true\")  \\\n",
    "      .json(\"Datasets (json)/article-db.json\")\n",
    "\n",
    "df_incollection = spark.read.option(\"multiline\",\"true\")  \\\n",
    "      .json(\"Datasets (json)/incollection-db.json\")\n",
    "\n",
    "df_www = spark.read.option(\"multiline\",\"true\")  \\\n",
    "      .json(\"Datasets (json)/www-db.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Most cited AUTHORS in a given field\n",
    "The following query can be used to find the authors, and their website (if existing), who\n",
    "wrote about a given field (e.g. \"data mining\") ordering them by the sum of citations they\n",
    "received for thoose publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df_article = df_article.filter(array_contains(col(\"keyword\"), \"data mining\"))  \\\n",
    "    .select(df_article.key, explode(df_article.author), df_article.keyword, df_article.citations  )    \\\n",
    "    .withColumnRenamed(\"col\", \"author\")\n",
    "# exploded_df_article.printSchema()\n",
    "# exploded_df_article.show()\n",
    "\n",
    "exploded_df_www = df_www  \\\n",
    "    .select(df_www.key, explode(df_www.author), df_www.url)    \\\n",
    "    .withColumnRenamed(\"col\", \"author\")\n",
    "# exploded_df_www.printSchema()\n",
    "# exploded_df_www.show()\n",
    "\n",
    "df_top_author = exploded_df_article.groupBy(\"author\").agg(\n",
    "    sum(\"citations\").alias(\"Sum of Citations\"), \n",
    "    avg(\"citations\").alias(\"Average Citations\"), \n",
    "    count(\"citations\").alias(\"Number of Paper\"),\n",
    "    max(\"citations\").alias(\"Max Citations\"))    \\\n",
    "    .sort(col(\"Sum of Citations\").desc()).limit(5)\n",
    "# df_top_author.printSchema()\n",
    "# df_top_author.show()\n",
    "\n",
    "df_website = df_www.select(explode(df_www.author), df_www.url)    \\\n",
    "    .withColumnRenamed(\"col\", \"author_w\")   \\\n",
    "    .withColumnRenamed(\"url\", \"website\")\n",
    "# df_website.printSchema()\n",
    "# df_website.show()\n",
    "\n",
    "\n",
    "result = df_top_author.join(df_website, df_top_author.author == df_website.author_w, \"left\")  \\\n",
    "    .drop(\"author_w\")\n",
    "    \n",
    "\n",
    "result.printSchema()    \n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add a KEYWORD to a publications\n",
    "here we can see a specific\n",
    "example with the binding of the keyword \"machine learning\" to books in the \"Intelligent\n",
    "Systems Reference Library\" series from volume 85 to volume 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show before update\n",
    "df_book\\\n",
    "    .filter(df_book.series.title == \"Intelligent Systems Reference Library\")\\\n",
    "    .filter(df_book.volume >= 85)\\\n",
    "    .filter(df_book.volume <= 100)\\\n",
    "    .select(\"key\",\"keyword\",\"volume\",df_book.series.title)\\\n",
    "    .sort(\"volume\")\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# filter and add a support column with the desired keyword\n",
    "df_book_update = df_book    \\\n",
    "    .withColumn(\"add\", \n",
    "        when( \n",
    "            (df_book.series.title == \"Intelligent Systems Reference Library\") & \n",
    "            (df_book.volume >= 85) & (df_book.volume <= 100),   \\\n",
    "            array(lit(\"machine learning\"))\n",
    "            )\\\n",
    "        .otherwise(array(lit(None)))\n",
    "        )\n",
    "\n",
    "# merge the keywords column with the suuport one, and then drop the latter\n",
    "df_book_update = df_book_update\\\n",
    "    .withColumn(\"keyword\", array_union(df_book_update.keyword, df_book_update.add))\\\n",
    "    .drop(\"add\")\n",
    "\n",
    "# Show after update\n",
    "df_book_update\\\n",
    "    .filter(df_book_update.series.title == \"Intelligent Systems Reference Library\")\\\n",
    "    .filter(df_book_update.volume >= 85)\\\n",
    "    .filter(df_book_update.volume <= 100)\\\n",
    "    .select(\"key\",\"keyword\",\"volume\", df_book_update.series.title)\\\n",
    "    .sort(\"volume\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add a new ARTICLE\n",
    "The following command can be used to Add a new article to volume 51 of journal \"Commun. ACM\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema of the element to insert\n",
    "schema= StructType([\n",
    "      # author field\n",
    "      StructField(\"author\",ArrayType(StructType([\n",
    "       StructField(\"orcid\", StringType(), True),\n",
    "       StructField(\"name\", StringType(), True)\n",
    "       ])),True),\n",
    "      # simple strings\n",
    "      StructField(\"key\",StringType(),True),\n",
    "      StructField(\"title\",StringType(),True),\n",
    "      StructField(\"publisher\",StringType(),True),\n",
    "      StructField(\"journal\",StringType(),True),\n",
    "      # array of strings\n",
    "      StructField(\"keyword\",ArrayType(StringType()), True),\n",
    "      StructField(\"ee\",ArrayType(StringType()), True),\n",
    "      StructField(\"cite\",ArrayType(StringType()), True),\n",
    "      StructField(\"note\",ArrayType(StringType()), True),\n",
    "      # simple integers\n",
    "      StructField(\"year\",LongType(),True),\n",
    "      StructField(\"citations\",LongType(),True),\n",
    "      StructField(\"pages\",LongType(),True),\n",
    "      StructField(\"volume\",LongType(),True),\n",
    "      ])\n",
    "\n",
    "# Upload the desired data\n",
    "data = [(\n",
    "            [(\"Author Orcid 1\",\"Author Name 1\"),(\"Author Orcid 2\",\"Author Name 2\")], \n",
    "            \n",
    "            \"NewArticleKey\", \n",
    "            \"Article Title\",\n",
    "            \"Publisher Name\",\n",
    "            \"Journal Name\",\n",
    "\n",
    "            [\"kw1\",\"kw2\"],\n",
    "            # ee = null \n",
    "            None,\n",
    "            # cite = null\n",
    "            None,\n",
    "            # note = null\n",
    "            None,\n",
    "\n",
    "            2022,\n",
    "            1,\n",
    "            10,\n",
    "            100\n",
    "      )]\n",
    "\n",
    "newArticle= spark.createDataFrame(data = data, schema = schema)\n",
    "df_article =df_article.unionByName(newArticle, allowMissingColumns=True)\n",
    "\n",
    "#check\n",
    "df_article.filter(df_article.key == \"NewArticleKey\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf259275ad4a72d4dd5b452264ad5fb2b635233dff2a31edc6ebc740e55e21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
